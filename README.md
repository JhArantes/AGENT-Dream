# âš™ï¸ğŸ’¬ AI Chat API â€” FastAPI + Strands Agents + Ollama

![AI Chat API](./IMGs/AI_Chat_IMG.png)

Este projeto implementa uma API de chat utilizando FastAPI, integrada a um Agente de IA local desenvolvido com o Strands Agents SDK e executado com modelos LLM via Ollama.

A aplicaÃ§Ã£o fornece um endpoint de conversaÃ§Ã£o com inteligÃªncia artificial, suporte a ferramentas personalizadas e arquitetura enxuta para uso local ou integraÃ§Ã£o com sistemas maiores.

O agente possui:
- ConversaÃ§Ã£o geral
- Tool personalizada para cÃ¡lculos matemÃ¡ticos
- IntegraÃ§Ã£o com FastAPI

---

## ğŸš€ Funcionalidades

- Endpoint `/chat` para conversaÃ§Ã£o
- Uso de modelo LLM local pelo Ollama
- Tool matemÃ¡tica que resolve expressÃµes
- ConfiguraÃ§Ã£o via `.env`
- Respostas rÃ¡pidas via FastAPI + Uvicorn

---

## ğŸ§± Requisitos

- Python 3.10+
- Ollama instalado
- Modelo local (ex.: `llama3.1`)

---

## ğŸ“¦ InstalaÃ§Ã£o

### 1. Instale o Ollama
Baixe em: https://ollama.com

### 2. Baixe o modelo desejado
```bash
ollama pull llama3.1
```

## ğŸ“¡ Uso do Endpoint /chat

### RequisiÃ§Ã£o

```bash
POST /chat
{
  "message": "Quanto Ã© 20 * (3 + 2)?"
}
```
### Resposta (exemplo)
```bash
{
  "response": "20 * (3 + 2) = 100"
}
```

## ğŸ› ï¸ Estrutura do Projeto

```bash
ğŸ“ app
 â”œâ”€â”€ agent.py        # ConfiguraÃ§Ã£o do Agente de IA
 â”œâ”€â”€ main.py         # API FastAPI
 â””â”€â”€ tools.py        # Tools customizadas (ex.: calculadora)
 
.env
requirements.txt
README.md
```